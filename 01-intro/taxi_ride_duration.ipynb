{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi ride duration prediction\n",
    "The goal of this homework is to train a simple model for predicting the duration of a ride - similar to what we did in this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Downloading the data\n",
    "\n",
    "We'll use [the same NYC taxi dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page),\n",
    "but instead of \"Green Taxi Trip Records\", we'll use \"For-Hire Vehicle Trip Records\".\n",
    "\n",
    "Download the data for January and February 2021.\n",
    "\n",
    "Note that you need \"For-Hire Vehicle Trip Records\", not \"High Volume For-Hire Vehicle Trip Records\".\n",
    "\n",
    "Read the data for January. How many records are there?\n",
    "\n",
    "* 1054112\n",
    "* 1154112\n",
    "* 1254112\n",
    "* 1354112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set needed data URLs and filenames\n",
    "jan_data_fname, jan_data_url = (\n",
    "    \"../local/data/jan_data.parquet\",\n",
    "    \"https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_2021-01.parquet\"\n",
    ")\n",
    "feb_data_fname, feb_data_url = (\n",
    "    \"../local/data/feb_data.parquet\",\n",
    "    \"https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_2021-02.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Download data\n",
    "urllib.request.urlretrieve(jan_data_url, jan_data_fname)\n",
    "urllib.request.urlretrieve(feb_data_url, feb_data_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from January\n",
    "jan_df = pd.read_parquet(jan_data_fname)\n",
    "n_records_original = len(jan_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many records are there? 1154112\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "print(f\"How many records are there? {n_records_original}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Computing duration\n",
    "\n",
    "Now let's compute the `duration` variable. It should contain the duration of a ride in minutes. \n",
    "\n",
    "What's the average trip duration in January?\n",
    "\n",
    "* 15.16\n",
    "* 19.16\n",
    "* 24.16\n",
    "* 29.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pick-up/drop-off times to datetime format and build new duration column\n",
    "jan_df.pickup_datetime = pd.to_datetime(jan_df.pickup_datetime)\n",
    "jan_df.dropOff_datetime = pd.to_datetime(jan_df.dropOff_datetime)\n",
    "jan_df[\"duration\"] = jan_df.dropOff_datetime - jan_df.pickup_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert duration from datetime to minutes\n",
    "jan_df.duration = jan_df.duration.apply(lambda td: td.total_seconds() / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the average trip duration in January? 19.167224093791006\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "print(f\"What's the average trip duration in January? {jan_df.duration.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Check the distribution of the duration variable. There are some outliers. \n",
    "\n",
    "Let's remove them and keep only the records where the duration was between 1 and 60 minutes (inclusive).\n",
    "\n",
    "How many records did you drop? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter outliers\n",
    "jan_df = jan_df[(jan_df.duration >= 1) & (jan_df.duration <= 60)]\n",
    "n_records_filtered = len(jan_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many records did you drop? 44286\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "print(f\"How many records did you drop? {n_records_original - n_records_filtered}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Missing values\n",
    "\n",
    "The features we'll use for our model are the pickup and dropoff location IDs. \n",
    "\n",
    "But they have a lot of missing values there. Let's replace them with \"-1\".\n",
    "\n",
    "What's the fractions of missing values for the pickup location ID? I.e. fraction of \"-1\"s after you filled the NAs.\n",
    "\n",
    "* 53%\n",
    "* 63%\n",
    "* 73%\n",
    "* 83%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Replace NaNs with -1\n",
    "jan_df.PUlocationID = jan_df.PUlocationID.fillna(-1)\n",
    "jan_df.DOlocationID = jan_df.DOlocationID.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of missing pick-up location values\n",
    "n_missing = (jan_df.PUlocationID == -1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many records did you drop? 83.52732770722618%\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "print(f\"How many records did you drop? {(n_missing / n_records_filtered) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. One-hot encoding\n",
    "\n",
    "Let's apply one-hot encoding to the pickup and dropoff location IDs. We'll use only these two features for our model. \n",
    "\n",
    "* Turn the dataframe into a list of dictionaries\n",
    "* Fit a dictionary vectorizer \n",
    "* Get a feature matrix from it\n",
    "\n",
    "What's the dimensionality of this matrix? (The number of columns).\n",
    "\n",
    "* 2\n",
    "* 152\n",
    "* 352\n",
    "* 525\n",
    "* 725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical values to strings\n",
    "categorical = [\"PUlocationID\", \"DOlocationID\"]\n",
    "jan_df[categorical] = jan_df[categorical].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn dataframe into list of dicts\n",
    "train_dict = jan_df[categorical].to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit dictionary vectorizer\n",
    "dict_vec = DictVectorizer()\n",
    "x_train = dict_vec.fit_transform(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the dimensionality of this matrix? 525\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "print(f\"What's the dimensionality of this matrix? {x_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Training a model\n",
    "\n",
    "Now let's use the feature matrix from the previous step to train a model. \n",
    "\n",
    "* Train a plain linear regression model with default parameters \n",
    "* Calculate the RMSE of the model on the training data\n",
    "\n",
    "What's the RMSE on train?\n",
    "\n",
    "* 5.52\n",
    "* 10.52\n",
    "* 15.52\n",
    "* 20.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ground truth data\n",
    "y_train = jan_df.duration.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Train a linear regressor\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure RMSE on training data\n",
    "y_pred = lr.predict(x_train)\n",
    "rmse = mean_squared_error(y_train, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the RMSE on train? 10.528519388409808\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "print(f\"What's the RMSE on train? {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Evaluating the model\n",
    "\n",
    "Now let's apply this model to the validation dataset (Feb 2021). \n",
    "\n",
    "What's the RMSE on validation?\n",
    "\n",
    "* 6.01\n",
    "* 11.01\n",
    "* 16.01\n",
    "* 21.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation data ready\n",
    "feb_df = pd.read_parquet(feb_data_fname)\n",
    "\n",
    "feb_df.pickup_datetime = pd.to_datetime(feb_df.pickup_datetime)\n",
    "feb_df.dropOff_datetime = pd.to_datetime(feb_df.dropOff_datetime)\n",
    "feb_df[\"duration\"] = feb_df.dropOff_datetime - feb_df.pickup_datetime\n",
    "feb_df.duration = feb_df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "feb_df = feb_df[(feb_df.duration >= 1) & (feb_df.duration <= 60)]\n",
    "\n",
    "feb_df.PUlocationID = feb_df.PUlocationID.fillna(-1)\n",
    "feb_df.DOlocationID = feb_df.DOlocationID.fillna(-1)\n",
    "\n",
    "feb_df[categorical] = feb_df[categorical].astype(str)\n",
    "\n",
    "val_dict = feb_df[categorical].to_dict(orient=\"records\")\n",
    "\n",
    "x_val = dict_vec.transform(val_dict)\n",
    "\n",
    "y_val = feb_df.duration.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DictVectorizer' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/mlops-zoomcamp-hw/01-intro/taxi_ride_duration.ipynb Cell 31'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmlops-zoomcamp/home/ubuntu/mlops-zoomcamp-hw/01-intro/taxi_ride_duration.ipynb#ch0000047vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# Measure RMSE on validation data\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmlops-zoomcamp/home/ubuntu/mlops-zoomcamp-hw/01-intro/taxi_ride_duration.ipynb#ch0000047vscode-remote?line=1'>2</a>\u001b[0m y_pred \u001b[39m=\u001b[39m lr\u001b[39m.\u001b[39;49mpredict(x_val)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmlops-zoomcamp/home/ubuntu/mlops-zoomcamp-hw/01-intro/taxi_ride_duration.ipynb#ch0000047vscode-remote?line=2'>3</a>\u001b[0m rmse \u001b[39m=\u001b[39m mean_squared_error(y_val, y_pred, squared\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/linear_model/_base.py:386\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/linear_model/_base.py?line=371'>372</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/linear_model/_base.py?line=372'>373</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/linear_model/_base.py?line=373'>374</a>\u001b[0m \u001b[39m    Predict using the linear model.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/linear_model/_base.py?line=374'>375</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/linear_model/_base.py?line=383'>384</a>\u001b[0m \u001b[39m        Returns predicted values.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/linear_model/_base.py?line=384'>385</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/linear_model/_base.py?line=385'>386</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decision_function(X)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/linear_model/_base.py:369\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/linear_model/_base.py?line=365'>366</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decision_function\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/linear_model/_base.py?line=366'>367</a>\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/linear_model/_base.py?line=368'>369</a>\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcoo\u001b[39;49m\u001b[39m\"\u001b[39;49m], reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/linear_model/_base.py?line=369'>370</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/base.py?line=574'>575</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/base.py?line=575'>576</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/base.py?line=576'>577</a>\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/base.py?line=577'>578</a>\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/base.py?line=578'>579</a>\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/utils/validation.py:806\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/utils/validation.py?line=802'>803</a>\u001b[0m context \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m estimator_name \u001b[39mif\u001b[39;00m estimator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/utils/validation.py?line=804'>805</a>\u001b[0m \u001b[39m# When all dataframe columns are sparse, convert to a sparse array\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/utils/validation.py?line=805'>806</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(array, \u001b[39m\"\u001b[39m\u001b[39msparse\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39;49mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/utils/validation.py?line=806'>807</a>\u001b[0m     \u001b[39m# DataFrame.sparse only supports `to_coo`\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/utils/validation.py?line=807'>808</a>\u001b[0m     array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39msparse\u001b[39m.\u001b[39mto_coo()\n\u001b[1;32m    <a href='file:///home/ubuntu/anaconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/sklearn/utils/validation.py?line=808'>809</a>\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DictVectorizer' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "# Measure RMSE on validation data\n",
    "y_pred = lr.predict(x_val)\n",
    "rmse = mean_squared_error(y_val, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "print(f\"What's the RMSE on validation? {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d05cbb977701fa1cced351e7617788d4c366745c1931f300760b6fbd246288cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('mlops-zoomcamp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
