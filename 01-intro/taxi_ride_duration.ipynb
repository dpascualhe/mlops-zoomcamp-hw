{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi ride duration prediction\n",
    "The goal of this homework is to train a simple model for predicting the duration of a ride - similar to what we did in this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Downloading the data\n",
    "\n",
    "We'll use [the same NYC taxi dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page),\n",
    "but instead of \"Green Taxi Trip Records\", we'll use \"For-Hire Vehicle Trip Records\".\n",
    "\n",
    "Download the data for January and February 2021.\n",
    "\n",
    "Note that you need \"For-Hire Vehicle Trip Records\", not \"High Volume For-Hire Vehicle Trip Records\".\n",
    "\n",
    "Read the data for January. How many records are there?\n",
    "\n",
    "* 1054112\n",
    "* 1154112\n",
    "* 1254112\n",
    "* 1354112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set needed data URLs and filenames\n",
    "jan_data_fname, jan_data_url = (\n",
    "    \"../local/data/jan_data.parquet\",\n",
    "    \"https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_2021-01.parquet\"\n",
    ")\n",
    "feb_data_fname, feb_data_url = (\n",
    "    \"../local/data/feb_data.parquet\",\n",
    "    \"https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_2021-02.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Download data\n",
    "urllib.request.urlretrieve(jan_data_url, jan_data_fname)\n",
    "urllib.request.urlretrieve(feb_data_url, feb_data_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from January\n",
    "jan_df = pd.read_parquet(jan_data_fname)\n",
    "n_records_original = len(jan_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many records are there? 1154112\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "print(f\"How many records are there? {n_records_original}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Computing duration\n",
    "\n",
    "Now let's compute the `duration` variable. It should contain the duration of a ride in minutes. \n",
    "\n",
    "What's the average trip duration in January?\n",
    "\n",
    "* 15.16\n",
    "* 19.16\n",
    "* 24.16\n",
    "* 29.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pick-up/drop-off times to datetime format and build new duration column\n",
    "jan_df.pickup_datetime = pd.to_datetime(jan_df.pickup_datetime)\n",
    "jan_df.dropOff_datetime = pd.to_datetime(jan_df.dropOff_datetime)\n",
    "jan_df[\"duration\"] = jan_df.dropOff_datetime - jan_df.pickup_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert duration from datetime to minutes\n",
    "jan_df.duration = jan_df.duration.apply(lambda td: td.total_seconds() / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the average trip duration in January? 19.167224093791006\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "print(f\"What's the average trip duration in January? {jan_df.duration.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Check the distribution of the duration variable. There are some outliers. \n",
    "\n",
    "Let's remove them and keep only the records where the duration was between 1 and 60 minutes (inclusive).\n",
    "\n",
    "How many records did you drop? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter outliers\n",
    "jan_df = jan_df[(jan_df.duration >= 1) & (jan_df.duration <= 60)]\n",
    "n_records_filtered = len(jan_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many records did you drop? 44286\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "print(f\"How many records did you drop? {n_records_original - n_records_filtered}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Missing values\n",
    "\n",
    "The features we'll use for our model are the pickup and dropoff location IDs. \n",
    "\n",
    "But they have a lot of missing values there. Let's replace them with \"-1\".\n",
    "\n",
    "What's the fractions of missing values for the pickup location ID? I.e. fraction of \"-1\"s after you filled the NAs.\n",
    "\n",
    "* 53%\n",
    "* 63%\n",
    "* 73%\n",
    "* 83%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Replace NaNs with -1\n",
    "jan_df.PUlocationID = jan_df.PUlocationID.fillna(-1)\n",
    "jan_df.DOlocationID = jan_df.DOlocationID.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of missing pick-up location values\n",
    "n_missing = (jan_df.PUlocationID == -1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many records did you drop? 83.52732770722618%\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "print(f\"How many records did you drop? {(n_missing / n_records_filtered) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. One-hot encoding\n",
    "\n",
    "Let's apply one-hot encoding to the pickup and dropoff location IDs. We'll use only these two features for our model. \n",
    "\n",
    "* Turn the dataframe into a list of dictionaries\n",
    "* Fit a dictionary vectorizer \n",
    "* Get a feature matrix from it\n",
    "\n",
    "What's the dimensionality of this matrix? (The number of columns).\n",
    "\n",
    "* 2\n",
    "* 152\n",
    "* 352\n",
    "* 525\n",
    "* 725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical values to strings\n",
    "categorical = [\"PUlocationID\", \"DOlocationID\"]\n",
    "jan_df[categorical] = jan_df[categorical].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn dataframe into list of dicts\n",
    "train_dict = jan_df[categorical].to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit dictionary vectorizer\n",
    "dict_vec = DictVectorizer()\n",
    "x_train = dict_vec.fit_transform(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the dimensionality of this matrix? 525\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "print(f\"What's the dimensionality of this matrix? {x_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Training a model\n",
    "\n",
    "Now let's use the feature matrix from the previous step to train a model. \n",
    "\n",
    "* Train a plain linear regression model with default parameters \n",
    "* Calculate the RMSE of the model on the training data\n",
    "\n",
    "What's the RMSE on train?\n",
    "\n",
    "* 5.52\n",
    "* 10.52\n",
    "* 15.52\n",
    "* 20.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ground truth data\n",
    "y_train = jan_df.duration.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Train a linear regressor\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure RMSE on training data\n",
    "y_pred = lr.predict(x_train)\n",
    "rmse = mean_squared_error(y_train, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the RMSE on train? 10.528519388409808\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "print(f\"What's the RMSE on train? {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Evaluating the model\n",
    "\n",
    "Now let's apply this model to the validation dataset (Feb 2021). \n",
    "\n",
    "What's the RMSE on validation?\n",
    "\n",
    "* 6.01\n",
    "* 11.01\n",
    "* 16.01\n",
    "* 21.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation data ready\n",
    "feb_df = pd.read_parquet(feb_data_fname)\n",
    "\n",
    "feb_df.pickup_datetime = pd.to_datetime(feb_df.pickup_datetime)\n",
    "feb_df.dropOff_datetime = pd.to_datetime(feb_df.dropOff_datetime)\n",
    "feb_df[\"duration\"] = feb_df.dropOff_datetime - feb_df.pickup_datetime\n",
    "feb_df.duration = feb_df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "feb_df = feb_df[(feb_df.duration >= 1) & (feb_df.duration <= 60)]\n",
    "\n",
    "feb_df.PUlocationID = feb_df.PUlocationID.fillna(-1)\n",
    "feb_df.DOlocationID = feb_df.DOlocationID.fillna(-1)\n",
    "\n",
    "feb_df[categorical] = feb_df[categorical].astype(str)\n",
    "\n",
    "val_dict = feb_df[categorical].to_dict(orient=\"records\")\n",
    "\n",
    "x_val = dict_vec.transform(val_dict)\n",
    "\n",
    "y_val = feb_df.duration.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure RMSE on validation data\n",
    "y_pred = lr.predict(x_val)\n",
    "rmse = mean_squared_error(y_val, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the RMSE on validation? 11.014287519486222\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "print(f\"What's the RMSE on validation? {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d05cbb977701fa1cced351e7617788d4c366745c1931f300760b6fbd246288cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('mlops-zoomcamp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
